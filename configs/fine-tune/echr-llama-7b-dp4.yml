# Fine-tune an LM on the enron text dataset.

dataset_args:
  # dataset_path: ../src/pii_leakage/extern/echr
  dataset_path: data/echr
  dataset_mode: undefended
  sample_duplication_rate: 1
  pseudonymize: False  # for non-scrubbing, disabling this can save time in data loading.

trainer_args:
  # output_dir: /storage/jyhong/projects/projects/PrivateLLM/pii_leakage/results/echr-gpt2-small-dp8/
  output_dir: ./results/echr-llama2-7b-dp4-4epochs/
  # evaluation_strategy: steps
  # eval_steps: 200
  logging_steps: 10
  save_total_limit: 2
  save_strategy: steps
  save_steps: 1000000 # avoid saving
  overwrite_output_dir: True
  callback_after_n_steps: 100
  num_train_epochs: 4
  # bs=32
  gradient_accumulation_steps: 8  # bs = gradient_accumulation_steps x #gpu x per_device_train_batch_size
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  remove_unused_columns: False
  # bf16: True
  # bf16_full_eval: True
  # NOTE remove fp16 if flash-attn error occurs
  fp16: True
  fp16_full_eval: True
  learning_rate: 0.00001
  resume_from_checkpoint: False
  deepspeed: configs/fine-tune/deepspeed_stage2.json
  do_train: True  # workaround to avoid moving model to device (useful, when you want to use auto device_map)
  # low_cpu_mem_usage: False

privacy_args:
  target_epsilon: 4

model_args:
  architecture: meta-llama/Llama-2-7b-hf
  pre_trained: True   # Start from a pre-trained checkpoint
  device_map: auto
  # peft
  peft: none
  # peft: lora  # none | lora
  # lora_r: 4  # default: 4

ner_args:
  ner: flair
  ner_model: flair/ner-english-ontonotes-large
  anon_token: <MASK>
  anonymize: False
